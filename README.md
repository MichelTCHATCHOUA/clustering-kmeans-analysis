# ğŸ¤– Clustering par l'Algorithme des K-Moyennes

Ce projet prÃ©sente une Ã©tude complÃ¨te de l'algorithme de **clustering non supervisÃ© K-means** appliquÃ© Ã  des donnÃ©es synthÃ©tiques et Ã  la base de donnÃ©es de chiffres manuscrits **Digits**. L'objectif est d'Ã©valuer les performances de l'algorithme, de dÃ©terminer le nombre optimal de clusters et d'utiliser les centroÃ¯des comme prototypes pour une tÃ¢che de classification.

---

## ğŸ¯ Objectif

L'Ã©tude vise Ã  maÃ®triser et analyser l'algorithme K-means Ã  travers plusieurs axes :

- Appliquer l'algorithme K-means sur des donnÃ©es synthÃ©tiques et mesurer son **coÃ»t (inertie intra-cluster)**.
- DÃ©terminer le **nombre optimal de clusters K** via la mÃ©thode du coude et le critÃ¨re de **Calinski-Harabasz**.
- Ã‰valuer la **stabilitÃ©** de l'algorithme Ã  travers plusieurs initialisations.
- Appliquer K-means Ã  la **reconnaissance de chiffres manuscrits** (base Digits) et mesurer la **puretÃ© des clusters**.
- Utiliser les centroÃ¯des K-means comme base d'apprentissage pour un classifieur **Plus-Proche-Voisin (PPV)**.
- ImplÃ©menter **manuellement** l'algorithme K-means et comparer les rÃ©sultats avec `sklearn`.

---

## ğŸ“Š DonnÃ©es

### DonnÃ©es synthÃ©tiques
- [**base1.txt**](donnÃ©es/base1.txt) : 300 points rÃ©partis en **3 classes rÃ©elles**, reprÃ©sentÃ©s en 2D â€” clusters bien sÃ©parÃ©s.
- [**base3.txt**](donnÃ©es/base3.txt) : 600 points rÃ©partis en **4 classes rÃ©elles**, reprÃ©sentÃ©s en 2D â€” structure plus complexe avec zones de recouvrement.

### Base Digits (sklearn)
- **Source** : `sklearn.datasets.load_digits`
- **Ã‰chantillon** : 1797 images de chiffres manuscrits (0 Ã  9)
- **Descripteurs** : 64 pixels par image (8Ã—8)
- **Classes** : 10 chiffres (0 Ã  9)
- **DÃ©coupage** : 70% apprentissage / 30% test via `train_test_split`

---

## ğŸ› ï¸ Outils utilisÃ©s

- **Python 3**
- **BibliothÃ¨ques** :
  - `numpy`, `pandas` â€” manipulation des donnÃ©es
  - `matplotlib`, `seaborn` â€” visualisation
  - `scikit-learn` â€” KMeans, KNeighborsClassifier, mÃ©triques, Digits
- **Environnement** : Jupyter Notebook

---

## âš™ï¸ PrÃ©requis et Installation

### 1. Cloner le dÃ©pÃ´t
```bash
git clone https://github.com/MichelTCHATCHOUA/clustering-kmeans-analysis
cd clustering-kmeans-analysis
```

### 2. CrÃ©er un environnement virtuel (recommandÃ©)
```bash
python -m venv venv
source venv/bin/activate        # Linux / macOS
venv\Scripts\activate           # Windows
```

### 3. Installer les dÃ©pendances
```bash
pip install -r requirements.txt
```

### `requirements.txt`
```
numpy
pandas
matplotlib
seaborn
scikit-learn
jupyter
```

---

## ğŸ“‚ Structure du projet
```text
TP2-Clustering-KMeans/
â”‚
â”œâ”€â”€ README.md                  # Ce fichier
â”œâ”€â”€ requirements.txt           # DÃ©pendances Python
â”œâ”€â”€ TP2_TCHATCHOUA.pdf         # Rapport complet d'analyse
â”œâ”€â”€ TP2_TCHATCHOUA.ipynb       # Notebook avec tous les calculs et graphiques
â”œâ”€â”€ base1.txt                  # DonnÃ©es synthÃ©tiques - 3 classes
â”œâ”€â”€ base3.txt                  # DonnÃ©es synthÃ©tiques - 4 classes
â””â”€â”€ images/                    # Visualisations gÃ©nÃ©rÃ©es
    â”œâ”€â”€ base1&base3_clusters.png
    â”œâ”€â”€ elbow_base1.png
    â”œâ”€â”€ elbow_base3.png
    â”œâ”€â”€ calinski_base1.png
    â”œâ”€â”€ calinski_base3.png
    â”œâ”€â”€ recognition_rate.png
    â””â”€â”€ kmeans_manual_vs_sklearn.png
```

---

## ğŸ”¬ MÃ©thodologie

L'analyse est structurÃ©e en quatre parties :

### 1. Ã‰valuation sur donnÃ©es synthÃ©tiques (base1 & base3)
- Application de K-means pour diffÃ©rentes valeurs de K.
- Observation visuelle de la distribution autour des centroÃ¯des.
- Calcul et comparaison du coÃ»t (inertie) selon K.

### 2. DÃ©termination du K optimal
- **5 initialisations** par valeur de K pour mesurer la stabilitÃ© (coÃ»t moyen et variance).
- **MÃ©thode du coude** : identification du point d'inflexion de la courbe d'inertie.
- **CritÃ¨re de Calinski-Harabasz** : maximisation du rapport variance inter/intra-cluster.
- VÃ©rification de la **convergence** des centroÃ¯des entre initialisations.

### 3. Application Ã  la reconnaissance de chiffres (Digits)
- **Clustering** : K=10 clusters, calcul de la **puretÃ©** par cluster.
- **Classification PPV** : les centroÃ¯des servent de prototypes, Ã©valuation du taux de reconnaissance pour k' = 1, 2, 3, 4 centroÃ¯des par classe.

### 4. ImplÃ©mentation manuelle
- Codage from scratch de l'algorithme K-means en Python.
- Comparaison des rÃ©sultats avec l'implÃ©mentation `sklearn`.

---

## ğŸ’¡ RÃ©sultats ClÃ©s

### DonnÃ©es synthÃ©tiques

| Base | K optimal | CoÃ»t moyen | Variance du coÃ»t | Score Calinski-Harabasz |
|------|-----------|------------|------------------|--------------------------|
| Base1 | **K = 3** | 18.50 | 0.000000 | ~1000 |
| Base3 | **K = 6** | 28.4 | ~0.000 | ~1415 |

- **Base1** : K = 3 correspond parfaitement aux 3 classes rÃ©elles, avec une stabilitÃ© excellente (variance nulle).
- **Base3** : bien que 4 classes rÃ©elles soient prÃ©sentes, K = 6 est recommandÃ© pour capturer les subdivisions internes.

### Clustering sur Digits (K = 10)

| MÃ©trique | Valeur |
|----------|--------|
| PuretÃ© Moyenne E[p] | **0.7760** |
| Variance de la PuretÃ© Var[p] | **0.000180** |

â¡ï¸ La faible variance confirme la **robustesse** du clustering sur plusieurs initialisations.

### Classification PPV avec centroÃ¯des K-means

| k' (centroÃ¯des/classe) | Taux moyen de reconnaissance | Variance |
|------------------------|------------------------------|----------|
| 1 | ~0.90 | 0.000000 |
| 2 | ~0.92 | 0.000042 |
| 3 | ~0.95 | 0.000002 |
| 4 | ~0.97 | 0.000038 |

â¡ï¸ L'augmentation de k' amÃ©liore le taux de reconnaissance en modÃ©lisant mieux la variabilitÃ© intra-classe.

### ImplÃ©mentation manuelle vs sklearn

| ImplÃ©mentation | Inertie Finale | StabilitÃ© |
|----------------|----------------|-----------|
| Manuelle | **18.4980** | âœ… OK |
| sklearn | **18.4980** | âœ… OK |

---

## ğŸ“Š Visualisations

### Distribution des clusters â€” Base1
Les donnÃ©es de `base1.txt` prÃ©sentent **3 clusters bien sÃ©parÃ©s**. la rÃ©partition des points montre une structure relativement claire composÃ©e de plusieurs groupes distincts, bien sÃ©parÃ©s les uns des autres.

![Clusters Base1&Base3](images/Base1_distribution.png)

Cette configuration suggÃ¨re que lâ€™algorithme des k-moyennes devrait parvenir Ã  regrouper efficacement les donnÃ©es avec un nombre de clusters modÃ©rÃ© (autour de 3 Ã  5).

---

### Distribution des clusters â€” Base3
Les donnÃ©es de `base3.txt` sont plus complexes, avec des zones de recouvrement. Cela rend le choix du nombre de clusters k plus dÃ©licat.  

![Clusters Base3](images/Base3_distribution.png)

Il se souligne donc lâ€™intÃ©rÃªt dâ€™utiliser des critÃ¨res objectifs tels que la mÃ©thode du coude ou le score de Calinski-Harabasz pour dÃ©terminer la valeur optimale de k.

---

### MÃ©thode du Coude & Score Calinski-Harabasz
Les deux critÃ¨res convergent vers les mÃªmes valeurs optimales :
- **Base1** : coude visible Ã  K = 3, score CH maximal Ã  K = 3.
- **Base3** : coude moins marquÃ©, score CH orientant vers K = 6.

<table>
  <tr>
    <td align="center">
      <img src="images/Coude_base1.png" width="400"><br>
      <em>MÃ©thode du coude â€” Base1</em>
    </td>
    <td align="center">
      <img src="images/CritÃ¨re_CH_base1.png" width="400"><br>
      <em>Score Calinski-Harabasz â€” Base1</em>
    </td>
  </tr>
</table>

<br>

<table>
  <tr>
    <td align="center">
      <img src="images/Coude_base3.png" width="400"><br>
      <em>MÃ©thode du coude â€” Base3</em>
    </td>
    <td align="center">
      <img src="images/CritÃ¨re_CH_base3.png" width="400"><br>
      <em>Score Calinski-Harabasz â€” Base3</em>
    </td>
  </tr>
</table>

---

### Taux de reconnaissance en fonction de k'
La courbe montre une progression du **taux de reconnaissance** Ã  mesure que le nombre de voisins *k'* augmente, avec une **faible variance** confirmant la stabilitÃ© des rÃ©sultats sur plusieurs initialisations.

![Taux de reconnaissance](images/Reconnaissance.png)

On observe que lâ€™augmentation de *k'* amÃ©liore progressivement la qualitÃ© de la classification, jusquâ€™Ã  atteindre une zone de **stabilisation** oÃ¹ les gains deviennent marginaux, ce qui indique un choix de *k'* robuste.

---

### Comparaison manuelle vs sklearn (K = 3)
Les deux implÃ©mentations produisent des **visualisations identiques** et une **inertie finale parfaitement concordante** (**18.4980**), validant la **correction de lâ€™implÃ©mentation manuelle** de lâ€™algorithme K-means.

![K-means manuel vs sklearn](images/LastFig.png)

Cette concordance confirme que lâ€™algorithme dÃ©veloppÃ© reproduit fidÃ¨lement le comportement dâ€™une implÃ©mentation de rÃ©fÃ©rence, garantissant la fiabilitÃ© des rÃ©sultats obtenus.

---

## ğŸ“ Algorithme K-means â€” Rappel

L'algorithme itÃ¨re entre deux Ã©tapes jusqu'Ã  convergence :

**Affectation** : chaque point est assignÃ© au cluster dont le centroÃ¯de est le plus proche.

$$
S_i^{(t)} = \lbrace \mathbf{x}_j : \|\mathbf{x}_j - \mathbf{m}_i^{(t)}\| \leq \|\mathbf{x}_j - \mathbf{m}_{i^\ast}^{(t)}\| \quad \forall \, i^\ast \rbrace
$$

**Mise Ã  jour** : recalcul du barycentre de chaque cluster.

$$\mathbf{m}_i^{(t+1)} = \frac{1}{|S_i^{(t)}|} \sum_{\mathbf{x}_j \in S_i^{(t)}} \mathbf{x}_j$$

**Fonction de coÃ»t minimisÃ©e** :

$$\sum_{i=1}^{k} \sum_{\mathbf{x}_j \in S_i} \|\mathbf{x}_j - \mathbf{m}_i\|^2$$

---

## ğŸ’» Code Source et Fichiers
*   **Notebook** : Le code complet pour le traitement des donnÃ©es, les calculs de variance et la gÃ©nÃ©ration des graphiques est disponible dans [codeSource.ipynb](TP2_TCHATCHOUA.ipynb).
*   **Rapport** : Pour une interprÃ©tation dÃ©taillÃ©e des courbes de densitÃ© et des matrices de covariance, consultez le [Rapport PDF](TP2_TCHATCHOUA.pdf).
*   
---

## ğŸ‘¤ Auteur

**Michel Peslier**
