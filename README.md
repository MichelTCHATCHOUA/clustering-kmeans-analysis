# ğŸ¤– Clustering par l'Algorithme des K-Moyennes

Ce projet prÃ©sente une Ã©tude complÃ¨te de l'algorithme de **clustering non supervisÃ© K-means** appliquÃ© Ã  des donnÃ©es synthÃ©tiques et Ã  la base de donnÃ©es de chiffres manuscrits **Digits**. L'objectif est d'Ã©valuer les performances de l'algorithme, de dÃ©terminer le nombre optimal de clusters et d'utiliser les centroÃ¯des comme prototypes pour une tÃ¢che de classification.

---

## ğŸ¯ Objectif

L'Ã©tude vise Ã  maÃ®triser et analyser l'algorithme K-means Ã  travers plusieurs axes :

- Appliquer l'algorithme K-means sur des donnÃ©es synthÃ©tiques et mesurer son **coÃ»t (inertie intra-cluster)**.
- DÃ©terminer le **nombre optimal de clusters K** via la mÃ©thode du coude et le critÃ¨re de **Calinski-Harabasz**.
- Ã‰valuer la **stabilitÃ©** de l'algorithme Ã  travers plusieurs initialisations.
- Appliquer K-means Ã  la **reconnaissance de chiffres manuscrits** (base Digits) et mesurer la **puretÃ© des clusters**.
- Utiliser les centroÃ¯des K-means comme base d'apprentissage pour un classifieur **Plus-Proche-Voisin (PPV)**.
- ImplÃ©menter **manuellement** l'algorithme K-means et comparer les rÃ©sultats avec `sklearn`.

---

## ğŸ“Š DonnÃ©es

### DonnÃ©es synthÃ©tiques
- **base1.txt** : 300 points rÃ©partis en **3 classes rÃ©elles**, reprÃ©sentÃ©s en 2D â€” clusters bien sÃ©parÃ©s.
- **base3.txt** : 600 points rÃ©partis en **4 classes rÃ©elles**, reprÃ©sentÃ©s en 2D â€” structure plus complexe avec zones de recouvrement.

### Base Digits (sklearn)
- **Source** : `sklearn.datasets.load_digits`
- **Ã‰chantillon** : 1797 images de chiffres manuscrits (0 Ã  9)
- **Descripteurs** : 64 pixels par image (8Ã—8)
- **Classes** : 10 chiffres (0 Ã  9)
- **DÃ©coupage** : 70% apprentissage / 30% test via `train_test_split`

---

## ğŸ› ï¸ Outils utilisÃ©s

- **Python 3**
- **BibliothÃ¨ques** :
  - `numpy`, `pandas` â€” manipulation des donnÃ©es
  - `matplotlib`, `seaborn` â€” visualisation
  - `scikit-learn` â€” KMeans, KNeighborsClassifier, mÃ©triques, Digits
- **Environnement** : Jupyter Notebook

---

## âš™ï¸ PrÃ©requis et Installation

### 1. Cloner le dÃ©pÃ´t
```bash
git clone https://github.com/MichelTCHATCHOUA/clustering-kmeans-analysis
cd clustering-kmeans-analysis
```

### 2. CrÃ©er un environnement virtuel (recommandÃ©)
```bash
python -m venv venv
source venv/bin/activate        # Linux / macOS
venv\Scripts\activate           # Windows
```

### 3. Installer les dÃ©pendances
```bash
pip install -r requirements.txt
```

### 4. Lancer le notebook
```bash
jupyter notebook [codeSource.ipynb](TP2_TCHATCHOUA.ipynb)
```

### `requirements.txt`
```
numpy
pandas
matplotlib
seaborn
scikit-learn
jupyter
```

---

## ğŸ“‚ Structure du projet
```text
TP2-Clustering-KMeans/
â”‚
â”œâ”€â”€ README.md                  # Ce fichier
â”œâ”€â”€ requirements.txt           # DÃ©pendances Python
â”œâ”€â”€ TP2_TCHATCHOUA.pdf         # Rapport complet d'analyse
â”œâ”€â”€ TP2_TCHATCHOUA.ipynb       # Notebook avec tous les calculs et graphiques
â”œâ”€â”€ base1.txt                  # DonnÃ©es synthÃ©tiques - 3 classes
â”œâ”€â”€ base3.txt                  # DonnÃ©es synthÃ©tiques - 4 classes
â””â”€â”€ images/                    # Visualisations gÃ©nÃ©rÃ©es
    â”œâ”€â”€ base1_clusters.png
    â”œâ”€â”€ base3_clusters.png
    â”œâ”€â”€ elbow_base1.png
    â”œâ”€â”€ elbow_base3.png
    â”œâ”€â”€ calinski_base1.png
    â”œâ”€â”€ calinski_base3.png
    â”œâ”€â”€ digits_purity.png
    â”œâ”€â”€ recognition_rate.png
    â””â”€â”€ kmeans_manual_vs_sklearn.png
```

---

## ğŸ”¬ MÃ©thodologie

L'analyse est structurÃ©e en quatre parties :

### 1. Ã‰valuation sur donnÃ©es synthÃ©tiques (base1 & base3)
- Application de K-means pour diffÃ©rentes valeurs de K.
- Observation visuelle de la distribution autour des centroÃ¯des.
- Calcul et comparaison du coÃ»t (inertie) selon K.

### 2. DÃ©termination du K optimal
- **5 initialisations** par valeur de K pour mesurer la stabilitÃ© (coÃ»t moyen et variance).
- **MÃ©thode du coude** : identification du point d'inflexion de la courbe d'inertie.
- **CritÃ¨re de Calinski-Harabasz** : maximisation du rapport variance inter/intra-cluster.
- VÃ©rification de la **convergence** des centroÃ¯des entre initialisations.

### 3. Application Ã  la reconnaissance de chiffres (Digits)
- **Clustering** : K=10 clusters, calcul de la **puretÃ©** par cluster.
- **Classification PPV** : les centroÃ¯des servent de prototypes, Ã©valuation du taux de reconnaissance pour k' = 1, 2, 3, 4 centroÃ¯des par classe.

### 4. ImplÃ©mentation manuelle
- Codage from scratch de l'algorithme K-means en Python.
- Comparaison des rÃ©sultats avec l'implÃ©mentation `sklearn`.

---

## ğŸ’¡ RÃ©sultats ClÃ©s

### DonnÃ©es synthÃ©tiques

| Base | K optimal | CoÃ»t moyen | Variance du coÃ»t | Score Calinski-Harabasz |
|------|-----------|------------|------------------|--------------------------|
| Base1 | **K = 3** | 18.50 | 0.000000 | ~1000 |
| Base3 | **K = 6** | 28.4 | ~0.000 | ~1415 |

- **Base1** : K = 3 correspond parfaitement aux 3 classes rÃ©elles, avec une stabilitÃ© excellente (variance nulle).
- **Base3** : bien que 4 classes rÃ©elles soient prÃ©sentes, K = 6 est recommandÃ© pour capturer les subdivisions internes.

### Clustering sur Digits (K = 10)

| MÃ©trique | Valeur |
|----------|--------|
| PuretÃ© Moyenne E[p] | **0.7760** |
| Variance de la PuretÃ© Var[p] | **0.000180** |

â¡ï¸ La faible variance confirme la **robustesse** du clustering sur plusieurs initialisations.

### Classification PPV avec centroÃ¯des K-means

| k' (centroÃ¯des/classe) | Taux moyen de reconnaissance | Variance |
|------------------------|------------------------------|----------|
| 1 | ~0.90 | 0.000000 |
| 2 | ~0.92 | 0.000042 |
| 3 | ~0.95 | 0.000002 |
| 4 | ~0.97 | 0.000038 |

â¡ï¸ L'augmentation de k' amÃ©liore le taux de reconnaissance en modÃ©lisant mieux la variabilitÃ© intra-classe.

### ImplÃ©mentation manuelle vs sklearn

| ImplÃ©mentation | Inertie Finale | StabilitÃ© |
|----------------|----------------|-----------|
| Manuelle | **18.4980** | âœ… OK |
| sklearn | **18.4980** | âœ… OK |

---

## ğŸ“Š Visualisations

### Distribution des clusters â€” Base1
Les donnÃ©es de `base1.txt` prÃ©sentent **3 clusters bien sÃ©parÃ©s**. Pour K = 3, l'algorithme converge vers une partition correspondant exactement aux classes rÃ©elles, avec une inertie finale de **18.50**.

> ğŸ“ Voir `images/base1_clusters.png`

---

### Distribution des clusters â€” Base3
Les donnÃ©es de `base3.txt` sont plus complexes, avec des zones de recouvrement. Pour K = 6, l'algorithme capture les subdivisions internes de certaines classes, donnant une partition cohÃ©rente.

> ğŸ“ Voir `images/base3_clusters.png`

---

### MÃ©thode du Coude & Score Calinski-Harabasz
Les deux critÃ¨res convergent vers les mÃªmes valeurs optimales :
- **Base1** : coude visible Ã  K = 3, score CH maximal Ã  K = 3.
- **Base3** : coude moins marquÃ©, score CH orientant vers K = 6.

> ğŸ“ Voir `images/elbow_base1.png`, `images/calinski_base1.png`, `images/elbow_base3.png`, `images/calinski_base3.png`

---

### Taux de reconnaissance en fonction de k'
La courbe montre une progression du taux de reconnaissance Ã  mesure que k' augmente, avec une variance faible confirmant la stabilitÃ© des rÃ©sultats sur plusieurs initialisations.

> ğŸ“ Voir `images/recognition_rate.png`

---

### Comparaison manuelle vs sklearn (K = 3)
Les deux implÃ©mentations produisent des visualisations identiques et une inertie finale parfaitement concordante (**18.4980**), validant la correction de l'implÃ©mentation manuelle.

> ğŸ“ Voir `images/kmeans_manual_vs_sklearn.png`

---

## ğŸ“ Algorithme K-means â€” Rappel

L'algorithme itÃ¨re entre deux Ã©tapes jusqu'Ã  convergence :

**Affectation** : chaque point est assignÃ© au cluster dont le centroÃ¯de est le plus proche.

$$S_i^{(t)} = \left\{ \mathbf{x}_j : \|\mathbf{x}_j - \mathbf{m}_i^{(t)}\| \leq \|\mathbf{x}_j - \mathbf{m}_{i^*}^{(t)}\| \; \forall \, i^* \right\}$$

**Mise Ã  jour** : recalcul du barycentre de chaque cluster.

$$\mathbf{m}_i^{(t+1)} = \frac{1}{|S_i^{(t)}|} \sum_{\mathbf{x}_j \in S_i^{(t)}} \mathbf{x}_j$$

**Fonction de coÃ»t minimisÃ©e** :

$$\sum_{i=1}^{k} \sum_{\mathbf{x}_j \in S_i} \|\mathbf{x}_j - \mathbf{m}_i\|^2$$

---

## ğŸ’» Rapport

* Pour une interprÃ©tation dÃ©taillÃ©e des courbes de densitÃ© et des matrices de covariance, consultez le [Rapport PDF](TP2_TCHATCHOUA.pdf).

---

## ğŸ‘¤ Auteur

**Michel Peslier**
