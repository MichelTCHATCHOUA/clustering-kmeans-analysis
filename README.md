# ü§ñ Clustering par l'Algorithme des K-Moyennes

Ce projet pr√©sente une √©tude compl√®te de l'algorithme de **clustering non supervis√© K-means** appliqu√© √† des donn√©es synth√©tiques et √† la base de donn√©es de chiffres manuscrits **Digits**. L'objectif est d'√©valuer les performances de l'algorithme, de d√©terminer le nombre optimal de clusters et d'utiliser les centro√Ødes comme prototypes pour une t√¢che de classification.

---

## üéØ Objectif

L'√©tude vise √† ma√Ætriser et analyser l'algorithme K-means √† travers plusieurs axes :

- Appliquer l'algorithme K-means sur des donn√©es synth√©tiques et mesurer son **co√ªt (inertie intra-cluster)**.
- D√©terminer le **nombre optimal de clusters K** via la m√©thode du coude et le crit√®re de **Calinski-Harabasz**.
- √âvaluer la **stabilit√©** de l'algorithme √† travers plusieurs initialisations.
- Appliquer K-means √† la **reconnaissance de chiffres manuscrits** (base Digits) et mesurer la **puret√© des clusters**.
- Utiliser les centro√Ødes K-means comme base d'apprentissage pour un classifieur **Plus-Proche-Voisin (PPV)**.
- Impl√©menter **manuellement** l'algorithme K-means et comparer les r√©sultats avec `sklearn`.

---

## üìä Donn√©es

### Donn√©es synth√©tiques
- **base1.txt** : 300 points r√©partis en **3 classes r√©elles**, repr√©sent√©s en 2D ‚Äî clusters bien s√©par√©s.
- **base3.txt** : 600 points r√©partis en **4 classes r√©elles**, repr√©sent√©s en 2D ‚Äî structure plus complexe avec zones de recouvrement.

### Base Digits (sklearn)
- **Source** : `sklearn.datasets.load_digits`
- **√âchantillon** : 1797 images de chiffres manuscrits (0 √† 9)
- **Descripteurs** : 64 pixels par image (8√ó8)
- **Classes** : 10 chiffres (0 √† 9)
- **D√©coupage** : 70% apprentissage / 30% test via `train_test_split`

---

## üõ†Ô∏è Outils utilis√©s

- **Python 3**
- **Biblioth√®ques** :
  - `numpy`, `pandas` ‚Äî manipulation des donn√©es
  - `matplotlib`, `seaborn` ‚Äî visualisation
  - `scikit-learn` ‚Äî KMeans, KNeighborsClassifier, m√©triques, Digits
- **Environnement** : Jupyter Notebook

---

## ‚öôÔ∏è Pr√©requis et Installation

### 1. Cloner le d√©p√¥t
```bash
git clone https://github.com/MichelTCHATCHOUA/clustering-kmeans-analysis
cd clustering-kmeans-analysis
```

### 2. Cr√©er un environnement virtuel (recommand√©)
```bash
python -m venv venv
source venv/bin/activate        # Linux / macOS
venv\Scripts\activate           # Windows
```

### 3. Installer les d√©pendances
```bash
pip install -r requirements.txt
```

### `requirements.txt`
```
numpy
pandas
matplotlib
seaborn
scikit-learn
jupyter
```

---

## üìÇ Structure du projet
```text
TP2-Clustering-KMeans/
‚îÇ
‚îú‚îÄ‚îÄ README.md                  # Ce fichier
‚îú‚îÄ‚îÄ requirements.txt           # D√©pendances Python
‚îú‚îÄ‚îÄ TP2_TCHATCHOUA.pdf         # Rapport complet d'analyse
‚îú‚îÄ‚îÄ TP2_TCHATCHOUA.ipynb       # Notebook avec tous les calculs et graphiques
‚îú‚îÄ‚îÄ base1.txt                  # Donn√©es synth√©tiques - 3 classes
‚îú‚îÄ‚îÄ base3.txt                  # Donn√©es synth√©tiques - 4 classes
‚îî‚îÄ‚îÄ images/                    # Visualisations g√©n√©r√©es
    ‚îú‚îÄ‚îÄ base1_clusters.png
    ‚îú‚îÄ‚îÄ base3_clusters.png
    ‚îú‚îÄ‚îÄ elbow_base1.png
    ‚îú‚îÄ‚îÄ elbow_base3.png
    ‚îú‚îÄ‚îÄ calinski_base1.png
    ‚îú‚îÄ‚îÄ calinski_base3.png
    ‚îú‚îÄ‚îÄ digits_purity.png
    ‚îú‚îÄ‚îÄ recognition_rate.png
    ‚îî‚îÄ‚îÄ kmeans_manual_vs_sklearn.png
```

---

## üî¨ M√©thodologie

L'analyse est structur√©e en quatre parties :

### 1. √âvaluation sur donn√©es synth√©tiques (base1 & base3)
- Application de K-means pour diff√©rentes valeurs de K.
- Observation visuelle de la distribution autour des centro√Ødes.
- Calcul et comparaison du co√ªt (inertie) selon K.

### 2. D√©termination du K optimal
- **5 initialisations** par valeur de K pour mesurer la stabilit√© (co√ªt moyen et variance).
- **M√©thode du coude** : identification du point d'inflexion de la courbe d'inertie.
- **Crit√®re de Calinski-Harabasz** : maximisation du rapport variance inter/intra-cluster.
- V√©rification de la **convergence** des centro√Ødes entre initialisations.

### 3. Application √† la reconnaissance de chiffres (Digits)
- **Clustering** : K=10 clusters, calcul de la **puret√©** par cluster.
- **Classification PPV** : les centro√Ødes servent de prototypes, √©valuation du taux de reconnaissance pour k' = 1, 2, 3, 4 centro√Ødes par classe.

### 4. Impl√©mentation manuelle
- Codage from scratch de l'algorithme K-means en Python.
- Comparaison des r√©sultats avec l'impl√©mentation `sklearn`.

---

## üí° R√©sultats Cl√©s

### Donn√©es synth√©tiques

| Base | K optimal | Co√ªt moyen | Variance du co√ªt | Score Calinski-Harabasz |
|------|-----------|------------|------------------|--------------------------|
| Base1 | **K = 3** | 18.50 | 0.000000 | ~1000 |
| Base3 | **K = 6** | 28.4 | ~0.000 | ~1415 |

- **Base1** : K = 3 correspond parfaitement aux 3 classes r√©elles, avec une stabilit√© excellente (variance nulle).
- **Base3** : bien que 4 classes r√©elles soient pr√©sentes, K = 6 est recommand√© pour capturer les subdivisions internes.

### Clustering sur Digits (K = 10)

| M√©trique | Valeur |
|----------|--------|
| Puret√© Moyenne E[p] | **0.7760** |
| Variance de la Puret√© Var[p] | **0.000180** |

‚û°Ô∏è La faible variance confirme la **robustesse** du clustering sur plusieurs initialisations.

### Classification PPV avec centro√Ødes K-means

| k' (centro√Ødes/classe) | Taux moyen de reconnaissance | Variance |
|------------------------|------------------------------|----------|
| 1 | ~0.90 | 0.000000 |
| 2 | ~0.92 | 0.000042 |
| 3 | ~0.95 | 0.000002 |
| 4 | ~0.97 | 0.000038 |

‚û°Ô∏è L'augmentation de k' am√©liore le taux de reconnaissance en mod√©lisant mieux la variabilit√© intra-classe.

### Impl√©mentation manuelle vs sklearn

| Impl√©mentation | Inertie Finale | Stabilit√© |
|----------------|----------------|-----------|
| Manuelle | **18.4980** | ‚úÖ OK |
| sklearn | **18.4980** | ‚úÖ OK |

---

## üìä Visualisations

### Distribution des clusters ‚Äî Base1
Les donn√©es de `base1.txt` pr√©sentent **3 clusters bien s√©par√©s**. Pour K = 3, l'algorithme converge vers une partition correspondant exactement aux classes r√©elles, avec une inertie finale de **18.50**.

> üìÅ Voir `images/base1_clusters.png`

---

### Distribution des clusters ‚Äî Base3
Les donn√©es de `base3.txt` sont plus complexes, avec des zones de recouvrement. Pour K = 6, l'algorithme capture les subdivisions internes de certaines classes, donnant une partition coh√©rente.

> üìÅ Voir `images/base3_clusters.png`

---

### M√©thode du Coude & Score Calinski-Harabasz
Les deux crit√®res convergent vers les m√™mes valeurs optimales :
- **Base1** : coude visible √† K = 3, score CH maximal √† K = 3.
- **Base3** : coude moins marqu√©, score CH orientant vers K = 6.

> üìÅ Voir `images/elbow_base1.png`, `images/calinski_base1.png`, `images/elbow_base3.png`, `images/calinski_base3.png`

---

### Taux de reconnaissance en fonction de k'
La courbe montre une progression du taux de reconnaissance √† mesure que k' augmente, avec une variance faible confirmant la stabilit√© des r√©sultats sur plusieurs initialisations.

> üìÅ Voir `images/recognition_rate.png`

---

### Comparaison manuelle vs sklearn (K = 3)
Les deux impl√©mentations produisent des visualisations identiques et une inertie finale parfaitement concordante (**18.4980**), validant la correction de l'impl√©mentation manuelle.

> üìÅ Voir `images/kmeans_manual_vs_sklearn.png`

---

## üìù Algorithme K-means ‚Äî Rappel

L'algorithme it√®re entre deux √©tapes jusqu'√† convergence :

**Affectation** : chaque point est assign√© au cluster dont le centro√Øde est le plus proche.

$$
S_i^{(t)} = \{ \mathbf{x}_j : \|\mathbf{x}_j - \mathbf{m}_i^{(t)}\| \le \|\mathbf{x}_j - \mathbf{m}_{i^*}^{(t)}\| \quad \forall i^* \}
$$

**Mise √† jour** : recalcul du barycentre de chaque cluster.

$$\mathbf{m}_i^{(t+1)} = \frac{1}{|S_i^{(t)}|} \sum_{\mathbf{x}_j \in S_i^{(t)}} \mathbf{x}_j$$

**Fonction de co√ªt minimis√©e** :

$$\sum_{i=1}^{k} \sum_{\mathbf{x}_j \in S_i} \|\mathbf{x}_j - \mathbf{m}_i\|^2$$

---

## üíª Code Source et Fichiers
*   **Notebook** : Le code complet pour le traitement des donn√©es, les calculs de variance et la g√©n√©ration des graphiques est disponible dans [codeSource.ipynb](TP2_TCHATCHOUA.ipynb).
*   **Rapport** : Pour une interpr√©tation d√©taill√©e des courbes de densit√© et des matrices de covariance, consultez le [Rapport PDF](TP2_TCHATCHOUA.pdf).
*   
---

## üë§ Auteur

**Michel Peslier**
